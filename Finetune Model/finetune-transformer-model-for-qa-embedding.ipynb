{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11929260,"sourceType":"datasetVersion","datasetId":7499768},{"sourceId":11957797,"sourceType":"datasetVersion","datasetId":7518475}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning SentenceTransformer to align text with graph embeddings (128D)\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:44:39.030989Z","iopub.execute_input":"2025-05-24T05:44:39.031305Z","iopub.status.idle":"2025-05-24T05:44:39.037213Z","shell.execute_reply.started":"2025-05-24T05:44:39.031282Z","shell.execute_reply":"2025-05-24T05:44:39.036185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# === Step 1: Load and preprocess data ===","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/qa-embeding/combined_doi_questions_embeddings.csv\")\nall_samples = []\n\nfor _, row in df.iterrows():\n    questions = str(row['questions']).split(';')\n    embedding = row[[str(i) for i in range(128)]].astype(float).values\n    for q in questions:\n        all_samples.append((q.strip(), embedding))\n\n\n# Group questions by id\nid_to_samples = {}\nfor _, row in df.iterrows():\n    questions = str(row['questions']).split(';')\n    embedding = row[[str(i) for i in range(128)]].astype(float).values\n    doc_id = row['id']\n    id_to_samples[doc_id] = [(q.strip(), embedding) for q in questions]\n\n# Split ids into train and test sets (1:9 ratio, test_size = 0.1)\nunique_ids = list(id_to_samples.keys())\ntrain_ids, test_ids = train_test_split(\n    unique_ids,\n    test_size=0.1,  # 1/(1+9) = 0.1 for 1:9 ratio\n    random_state=42  # For reproducibility\n)\n\n# Create train and test samples based on id split\ntrain_samples = [sample for doc_id in train_ids for sample in id_to_samples[doc_id]]\ntest_samples = [sample for doc_id in test_ids for sample in id_to_samples[doc_id]]\n\nprint(f\"Total samples: {len(all_samples)}\")\nprint(f\"Training samples: {len(train_samples)} ({len(train_samples)/len(all_samples)*100:.1f}%)\")\nprint(f\"Test samples: {len(test_samples)} ({len(test_samples)/len(all_samples)*100:.1f}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:44:39.291852Z","iopub.execute_input":"2025-05-24T05:44:39.292106Z","iopub.status.idle":"2025-05-24T05:44:47.306057Z","shell.execute_reply.started":"2025-05-24T05:44:39.292086Z","shell.execute_reply":"2025-05-24T05:44:47.305147Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Class","metadata":{}},{"cell_type":"code","source":"class QuestionGraphDataset(Dataset):\n    def __init__(self, samples, tokenizer, max_len=64):\n        self.samples = samples\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        question, target_embedding = self.samples[idx]\n        encoded = self.tokenizer(question, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n        return {\n            'input_ids': encoded['input_ids'].squeeze(0),\n            'attention_mask': encoded['attention_mask'].squeeze(0),\n            'target': torch.tensor(target_embedding, dtype=torch.float)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:44:47.308262Z","iopub.execute_input":"2025-05-24T05:44:47.308560Z","iopub.status.idle":"2025-05-24T05:44:47.314223Z","shell.execute_reply.started":"2025-05-24T05:44:47.308538Z","shell.execute_reply":"2025-05-24T05:44:47.312933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class QuestionEncoder(nn.Module):\n    def __init__(self, pretrained_model, out_dim=128):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(pretrained_model)\n        self.projection = nn.Linear(self.encoder.config.hidden_size, out_dim)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n        projected = self.projection(cls_output)\n        return projected","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training class","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_dataloader, test_dataloader, epochs=200, lr=2e-5, device='cuda' if torch.cuda.is_available() else 'cpu', save_path=\"question_encoder.pt\"):\n    model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        total_train_loss = 0.0\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1} (Train)\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            target = batch['target'].to(device)\n\n            optimizer.zero_grad()\n            output = model(input_ids, attention_mask)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n            total_train_loss += loss.item()\n\n        avg_train_loss = total_train_loss / len(train_dataloader)\n\n        # Evaluate on test set\n        model.eval()\n        total_test_loss = 0.0\n        all_outputs = []\n        all_targets = []\n        with torch.no_grad():\n            for batch in test_dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                target = batch['target'].to(device)\n\n                output = model(input_ids, attention_mask)\n                loss = criterion(output, target)\n                total_test_loss += loss.item()\n\n                all_outputs.append(output.cpu().numpy())\n                all_targets.append(target.cpu().numpy())\n\n        avg_test_loss = total_test_loss / len(test_dataloader)\n\n        # Compute cosine similarity\n        all_outputs = np.concatenate(all_outputs, axis=0)\n        all_targets = np.concatenate(all_targets, axis=0)\n        cosine_sim = np.mean([cosine_similarity([output], [target])[0][0] for output, target in zip(all_outputs, all_targets)])\n\n        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, Test Cosine Similarity: {cosine_sim:.4f}\")\n\n    # Save model\n    torch.save(model.state_dict(), save_path)\n    print(f\"✅ Model saved to {save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:44:47.347494Z","iopub.execute_input":"2025-05-24T05:44:47.348200Z","iopub.status.idle":"2025-05-24T05:44:47.403679Z","shell.execute_reply.started":"2025-05-24T05:44:47.348176Z","shell.execute_reply":"2025-05-24T05:44:47.402744Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Eval Function","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, test_dataloader, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    model.to(device)\n    model.eval()\n    criterion = nn.MSELoss()\n    total_test_loss = 0.0\n    all_outputs = []\n    all_targets = []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            target = batch['target'].to(device)\n\n            output = model(input_ids, attention_mask)\n            loss = criterion(output, target)\n            total_test_loss += loss.item()\n\n            all_outputs.append(output.cpu().numpy())\n            all_targets.append(target.cpu().numpy())\n\n    avg_test_loss = total_test_loss / len(test_dataloader)\n    all_outputs = np.concatenate(all_outputs, axis=0)\n    all_targets = np.concatenate(all_targets, axis=0)\n    cosine_sim = np.mean([cosine_similarity([output], [target])[0][0] for output, target in zip(all_outputs, all_targets)])\n\n    print(f\"Evaluation - Test MSE Loss: {avg_test_loss:.4f}, Test Cosine Similarity: {cosine_sim:.4f}\")\n    return avg_test_loss, cosine_sim","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finetune model","metadata":{}},{"cell_type":"code","source":"model_names = [ \"intfloat/e5-large-v2\"]\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nfor model_name in model_names:\n    print(f\"\\n=== Fine-Tuning {model_name} ===\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Create datasets and dataloaders\n    train_dataset = QuestionGraphDataset(train_samples, tokenizer, max_len=64)\n    test_dataset = QuestionGraphDataset(test_samples, tokenizer, max_len=64)\n    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n    # Initialize and train model\n    model = QuestionEncoder(pretrained_model=model_name, out_dim=128)\n    save_path = f\"{model_name.split('/')[-1]}_question_encoder.pt\"\n    train_model(model, train_dataloader, test_dataloader, epochs=400, lr=2e-5, save_path=save_path)\n\n    # Evaluate model\n    print(f\"\\n=== Evaluating {model_name} ===\")\n    evaluate_model(model, test_dataloader)\n\n    # === Query Inference ===\n    model.eval()\n    query = \"How do the studies of neutrino oscillations in the Sudbury Neutrino Observatory (SNO), the strong coupling dynamics of the standard Higgs sector, and the evolution of color exchange in QCD hard scattering collectively contribute to advancing our understanding of fundamental particle interactions and their implications for experimental observations at high-energy facilities like the LHC?\"\n    encoded = tokenizer(query, return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n    encoded = {k: v.to(device) for k, v in encoded.items()}\n\n    with torch.no_grad():\n        query_vector = model(encoded['input_ids'], encoded['attention_mask']).cpu().numpy()\n\n    # Compute cosine similarity with DOI embeddings\n    doi_embeddings = df[[str(i) for i in range(128)]].values\n    doi_ids = df['id'].tolist()\n    similarities = cosine_similarity(query_vector, doi_embeddings)[0]\n    top_indices = np.argsort(similarities)[::-1][:5]\n    top_dois = [doi_ids[i] for i in top_indices]\n    top_scores = [similarities[i] for i in top_indices]\n\n    print(f\"\\n🔍 Top 5 closest papers for {model_name}:\")\n    for rank, (doi, score) in enumerate(zip(top_dois, top_scores), 1):\n        print(f\"{rank}. DOI: {doi} — Similarity: {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:44:47.648571Z","iopub.execute_input":"2025-05-24T05:44:47.648881Z","iopub.status.idle":"2025-05-24T05:44:54.896451Z","shell.execute_reply.started":"2025-05-24T05:44:47.648854Z","shell.execute_reply":"2025-05-24T05:44:54.895147Z"}},"outputs":[],"execution_count":null}]}